# Framework Declarativo para Pipelines de Dados no Databricks

1. VisÃ£o Geral

Este framework foi projetado para acelerar e padronizar o desenvolvimento de pipelines de dados (ETL/ELT) no Databricks. A abordagem Ã© declarativa: em vez de escrever cÃ³digo PySpark ou SQL repetitivo para cada pipeline, vocÃª declara as fontes, destinos, transformaÃ§Ãµes e regras de qualidade em arquivos de configuraÃ§Ã£o YAML.

O motor do framework lÃª esses arquivos e executa as operaÃ§Ãµes correspondentes, garantindo uniformidade, qualidade e otimizando o tempo de desenvolvimento.

Principais CaracterÃ­sticas
ConfiguraÃ§Ã£o via YAML: Defina pipelines de forma simples e legÃ­vel.

Suporte a MÃºltiplos Motores: Escolha entre executar pipelines com Spark (via script PySpark) ou gerar cÃ³digo otimizado para Delta Live Tables (DLT).

PadrÃ£o de Camadas (Medallion): Modelos de configuraÃ§Ã£o direcionados para as camadas Silver (limpeza e qualidade) e Gold (agregaÃ§Ã£o e regras de negÃ³cio).

Qualidade de Dados Embutida: Defina validaÃ§Ãµes (constraints) diretamente nas colunas, que sÃ£o aplicadas automaticamente.

ExtensÃ­vel: FÃ¡cil de adicionar novas transformaÃ§Ãµes e regras de validaÃ§Ã£o.

2. Estrutura de DiretÃ³rios
O projeto Ã© organizado da seguinte forma para manter o cÃ³digo e as configuraÃ§Ãµes separadas e de fÃ¡cil manutenÃ§Ã£o.

/
â”œâ”€â”€ ğŸ“‚ pipelines/             # Arquivos de configuraÃ§Ã£o YAML para cada pipeline.
â”‚   â”œâ”€â”€ silver_contas.yaml
â”‚   â””â”€â”€ gold_clientes_enriquecidos.yaml
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â””â”€â”€ ğŸ“‚ framework/         # O cÃ³digo-fonte do motor do framework.
â”‚       â”œâ”€â”€ engine.py         # Motor principal que orquestra a execuÃ§Ã£o.
â”‚       â”œâ”€â”€ steps_handler.py  # LÃ³gica de transformaÃ§Ã£o para o motor Spark.
â”‚       â”œâ”€â”€ quality.py        # LÃ³gica de validaÃ§Ã£o para o motor Spark.
â”‚       â””â”€â”€ dlt_generator.py  # Gerador de scripts SQL para DLT.
â”‚
â”œâ”€â”€ ğŸ“‚ dlt_generated/         # (Criado automaticamente) Scripts SQL gerados para DLT.
â”‚
â”œâ”€â”€ ğŸ““ Run_Pipeline.py        # Notebook Databricks para executar o framework.
â””â”€â”€ ğŸ“ requirements.txt      # DependÃªncias Python.

3. Como Usar
O fluxo de trabalho consiste em duas etapas principais: configurar o pipeline no YAML e executÃ¡-lo atravÃ©s do notebook.

Etapa 1: Configurar o Pipeline (YAML)
Crie um novo arquivo .yaml na pasta /pipelines. A estrutura do YAML depende da camada (Silver ou Gold) e do motor de execuÃ§Ã£o escolhido (spark ou dlt).

Exemplo: Pipeline Silver
Ideal para ingestÃ£o, limpeza, tipagem, padronizaÃ§Ã£o e validaÃ§Ã£o dos dados brutos.

# pipelines/silver_contas.yaml
engine: "spark" # ou "dlt"
pipeline_type: "silver"
pipeline_name: "silver_contas"
description: "Ingere dados de contas, aplica qualidade e padronizaÃ§Ã£o."

source:
  format: "cloudFiles"
  path: "/mnt/raw/contas/contas_*.csv"
  options:
    cloudFiles.format: "csv"
    header: "true"

sink:
  catalog: "dev"
  schema: "silver"
  table: "contas"
  mode: "append"

columns:
  - name: "account_id"          # Nome original da coluna na fonte
    rename: "id_conta"           # Novo nome da coluna
    type: "long"                 # Tipo de dado final
    description: "ID da conta."
    pk: true                     # Informativo: indica chave primÃ¡ria
    validate:                    # Regras de qualidade
      - "not_null"

  - name: "account_type"
    rename: "tipo_conta"
    type: "string"
    transform: "UPPER(TRIM(account_type))" # ExpressÃ£o SQL para transformar o valor
    validate:
      - "isin:['CORRENTE', 'POUPANCA']"

  - name: "balance"
    rename: "saldo"
    type: "decimal(18, 2)"
    validate:
      - "greater_than_or_equal_to:0"

Exemplo: Pipeline Gold
Ideal para criar tabelas agregadas e visÃµes de negÃ³cio, juntando dados de uma ou mais tabelas da camada Silver.

# pipelines/gold_clientes_enriquecidos.yaml
engine: "dlt" # ou "spark"
pipeline_type: "gold"
pipeline_name: "gold_clientes_enriquecidos"
description: "Cria uma visÃ£o 360 de clientes com dados de contas."

dependencies: # Tabelas da camada Silver necessÃ¡rias
  - "dev.silver.clientes"
  - "dev.silver.contas"

sink:
  catalog: "dev"
  schema: "gold"
  table: "clientes_enriquecidos"
  mode: "overwrite"

transformation:
  type: "sql" # Ãšnico tipo suportado para Gold no momento
  sql: |
    SELECT
      c.id_cliente,
      c.nome_completo,
      COUNT(a.id_conta) AS quantidade_contas,
      SUM(a.saldo) AS saldo_total_consolidado
    FROM dev.silver.clientes c
    LEFT JOIN dev.silver.contas a ON c.id_cliente = a.id_cliente
    GROUP BY c.id_cliente, c.nome_completo

Etapa 2: Executar o Pipeline
Use o notebook Run_Pipeline.py para iniciar a execuÃ§Ã£o.

Abra o notebook no Databricks.

No widget pipeline_config na parte superior, insira o caminho relativo para o seu arquivo de configuraÃ§Ã£o (ex: pipelines/silver_contas.yaml).

Execute o notebook.

Comportamento por Motor
engine: "spark": O notebook executarÃ¡ o pipeline PySpark imediatamente. Os logs da execuÃ§Ã£o aparecerÃ£o no output do notebook.

engine: "dlt": O notebook nÃ£o executarÃ¡ o pipeline. Em vez disso, ele irÃ¡ gerar um novo arquivo .sql na pasta /dlt_generated. O output do notebook informarÃ¡ o caminho para este arquivo e os prÃ³ximos passos para configurar seu pipeline na interface do Delta Live Tables no Databricks.


4. Extensibilidade
O framework foi construÃ­do para ser extensÃ­vel. Para adicionar novas funcionalidades:

## Custom Validation Rules
You can register or overwrite custom validation rules without modifying the library code. Use the `register_rule` function:

```python
from declarative_data_framework.quality import register_rule

class MyCustomValidation:
  def __init__(self, params: dict = None):
    # Your initialization logic
    pass
  def apply(self, df, column_name):
    # Your validation logic
    return failures_df, success_df

# Register your rule (overwrites if the name already exists)
register_rule("my_custom_rule", MyCustomValidation)
```

You can also use it as a decorator:

```python
from declarative_data_framework.quality import register_rule

@register_rule("my_custom_rule")
class MyCustomValidation:
  ...
```

After registration, use your rule in the pipeline YAML as usual.

Novas ValidaÃ§Ãµes (Spark): Adicione uma nova lÃ³gica condicional no arquivo src/framework/quality.py.

Novas ValidaÃ§Ãµes (DLT): Adicione um novo gerador de CONSTRAINT no arquivo src/framework/dlt_generator.py.

Novos Tipos de TransformaÃ§Ã£o: Adicione novas funÃ§Ãµes no arquivo src/framework/steps_handler.py (para o motor Spark).